{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import nltk\n",
    "import string\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_train = pd.read_csv(\"../input/drugsComTest_raw.csv\",encoding='latin1')\n",
    "df_test =  pd.read_csv(\"../input/drugsComTrain_raw.csv\",encoding='latin1')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (\"The shape of the train set given is : \", df_train.shape)\n",
    "print (\"The shape of the test set given is : \", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping the data points with null values as it's very much less tha 5% of the whole dataset\n",
    "df_train = df_train.dropna(how = 'any', axis = 0)\n",
    "df_test = df_test.dropna(how = 'any', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"The shape of the train df :\",df_train.shape)\n",
    "print (\"The shape of the test df :\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the test and train data \n",
    "merge = [df_train, df_test]\n",
    "df_data = pd.concat(merge)\n",
    "df_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_data.columns = df_data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sorting the dataframe\n",
    "df_data.sort_values(['uniqueid'], ascending = True, inplace = True)\n",
    "df_data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['condition'].isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Converting the date in to date time format\n",
    "df_data['date'] = pd.to_datetime(df_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 unigrams according to the rating\n",
    "df_ = df_data[['rating', 'review']]\n",
    "df_['review'] = df_data['review'].str.replace(\"&#039;\", \"\")\n",
    "df_['review'] = df_['review'].str.replace(r'[^\\w\\d\\s]',' ')\n",
    "\n",
    "df_review_5 = \" \".join(df_.loc[df_.rating <= 5, 'review'])\n",
    "df_review_10 = \" \".join(df_.loc[df_.rating > 5, 'review'])\n",
    "\n",
    "token_review_5 = word_tokenize(df_review_5)\n",
    "token_review_10 = word_tokenize(df_review_10)\n",
    "\n",
    "unigrams_5 = ngrams(token_review_5, 1)\n",
    "unigrams_10 = ngrams(token_review_10, 1)\n",
    "\n",
    "frequency_5 = Counter(unigrams_5)\n",
    "frequency_10 = Counter(unigrams_10)\n",
    "\n",
    "df_5 = pd.DataFrame(frequency_5.most_common(20))\n",
    "df_10 = pd.DataFrame(frequency_10.most_common(20))\n",
    "\n",
    "# Barplot that shows the top 20 unigrams\n",
    "plt.rcParams['figure.figsize'] = [20,11]\n",
    "fig, ax = plt.subplots(1,2)\n",
    "sns.set(font_scale = 1.5, style = 'whitegrid')\n",
    "\n",
    "sns_5 = sns.barplot(x = df_5[1], y = df_5[0], color = 'lightsteelblue', ax = ax[0])\n",
    "sns_10 = sns.barplot(x = df_10[1], y = df_10[0], color = 'lightsteelblue', ax = ax[1])\n",
    "\n",
    "# Setting axes labels\n",
    "sns_5.set_title(\"Top 20 unigrams according for rating <= 5\")\n",
    "sns_10.set_title(\"Top 20 unigrams according for rating > 5\")\n",
    "sns_5.set_ylabel(\"Unigrams\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 bigrams according to the rating\n",
    "bigrams_5 = ngrams(token_review_5, 2)\n",
    "bigrams_10 = ngrams(token_review_10, 2)\n",
    "\n",
    "frequency_5 = Counter(bigrams_5)\n",
    "frequency_10 = Counter(bigrams_10)\n",
    "\n",
    "df_5 = pd.DataFrame(frequency_5.most_common(20))\n",
    "df_10 = pd.DataFrame(frequency_10.most_common(20))\n",
    "\n",
    "# Barplot that shows the top 20 bigrams\n",
    "plt.rcParams['figure.figsize'] = [22,11]\n",
    "fig, ax = plt.subplots(1,2)\n",
    "sns.set(font_scale = 1.3, style = 'whitegrid')\n",
    "\n",
    "sns_5 = sns.barplot(x = df_5[1], y = df_5[0], color = 'red', ax = ax[0])\n",
    "sns_10 = sns.barplot(x = df_10[1], y = df_10[0], color = 'red', ax = ax[1])\n",
    "\n",
    "# Setting axes labels\n",
    "sns_5.set_title(\"Top 20 bigrams according for rating <= 5\")\n",
    "sns_10.set_title(\"Top 20 bigrams according for rating > 5\")\n",
    "sns_5.set_ylabel(\"bigrams\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 trigrams according to the rating\n",
    "trigrams_5 = ngrams(token_review_5, 3)\n",
    "trigrams_10 = ngrams(token_review_10, 3)\n",
    "\n",
    "frequency_5 = Counter(trigrams_5)\n",
    "frequency_10 = Counter(trigrams_10)\n",
    "\n",
    "df_5 = pd.DataFrame(frequency_5.most_common(20))\n",
    "df_10 = pd.DataFrame(frequency_10.most_common(20))\n",
    "\n",
    "# Barplot that shows the top 20 trigrams\n",
    "plt.rcParams['figure.figsize'] = [25,13]\n",
    "fig, ax = plt.subplots(1,2)\n",
    "sns.set(font_scale = 1.3, style = 'whitegrid')\n",
    "\n",
    "sns_5 = sns.barplot(x = df_5[1], y = df_5[0], color = 'orange', ax = ax[0])\n",
    "sns_10 = sns.barplot(x = df_10[1], y = df_10[0], color = 'orange', ax = ax[1])\n",
    "\n",
    "# Setting axes labels\n",
    "sns_5.set_title(\"Top 20 trigrams according for rating <= 5\")\n",
    "sns_10.set_title(\"Top 20 trigrams according for rating > 5\")\n",
    "sns_5.set_ylabel(\"trigrams\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving the Sentiment according to the ratings\n",
    "df_data['sentiment_rate'] = df_data['rating'].apply(lambda x: 1 if x > 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def review_clean(review): \n",
    "    # changing to lower case\n",
    "    lower = review.str.lower()\n",
    "    \n",
    "    # Replacing the repeating pattern of &#039;\n",
    "    pattern_remove = lower.str.replace(\"&#039;\", \"\")\n",
    "    \n",
    "    # Removing all the special Characters\n",
    "    special_remove = pattern_remove.str.replace(r'[^\\w\\d\\s]',' ')\n",
    "    \n",
    "    # Removing all the non ASCII characters\n",
    "    ascii_remove = special_remove.str.replace(r'[^\\x00-\\x7F]+',' ')\n",
    "    \n",
    "    # Removing the leading and trailing Whitespaces\n",
    "    whitespace_remove = ascii_remove.str.replace(r'^\\s+|\\s+?$','')\n",
    "    \n",
    "    # Replacing multiple Spaces with Single Space\n",
    "    multiw_remove = whitespace_remove.str.replace(r'\\s+',' ')\n",
    "    \n",
    "    # Replacing Two or more dots with one\n",
    "    dataframe = multiw_remove.str.replace(r'\\.{2,}', ' ')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data['review_clean'] = df_data['review'].apply(review_clean)\n",
    "df_data['review_clean'] = review_clean(df_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df_data['review_clean'] = df_data['review_clean'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the word stems using the Snowball Stemmer\n",
    "Snow_ball = SnowballStemmer(\"english\")\n",
    "df_data['review_clean'] = df_data['review_clean'].apply(lambda x: \" \".join(Snow_ball.stem(word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['review_clean'][0: 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the day, month and year from the Date\n",
    "df_data['day'] = df_data['date'].dt.day\n",
    "df_data['month'] = df_data['date'].dt.month\n",
    "df_data['year'] = df_data['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(review):\n",
    "    # Sentiment polarity of the reviews\n",
    "    pol = []\n",
    "    for i in review:\n",
    "        analysis = TextBlob(i)\n",
    "        pol.append(analysis.sentiment.polarity)\n",
    "    return pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['sentiment'] = sentiment(df_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['sentiment_clean'] = sentiment(df_data['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(df_data['sentiment'], df_data['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.corrcoef(df_data['sentiment_clean'], df_data['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the reviews without removing the stop words and using snowball stemmer\n",
    "df_data['review_clean_ss'] = review_clean(df_data['review'])\n",
    "df_data['sentiment_clean_ss'] = sentiment(df_data['review_clean_ss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.corrcoef(df_data['sentiment_clean_ss'], df_data['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['condition'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "df_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word count in each review\n",
    "df_data['count_word']=df_data[\"review_clean_ss\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#Unique word count \n",
    "df_data['count_unique_word']=df_data[\"review_clean_ss\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "#Letter count\n",
    "df_data['count_letters']=df_data[\"review_clean_ss\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "#punctuation count\n",
    "df_data[\"count_punctuations\"] = df_data[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "#upper case words count\n",
    "df_data[\"count_words_upper\"] = df_data[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "#title case words count\n",
    "df_data[\"count_words_title\"] = df_data[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "#Number of stopwords\n",
    "df_data[\"count_stopwords\"] = df_data[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
    "\n",
    "#Average length of the words\n",
    "df_data[\"mean_word_len\"] = df_data[\"review_clean_ss\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap of the features engineered\n",
    "plt.rcParams['figure.figsize'] = [17,15]\n",
    "sns.set(font_scale = 1.2)\n",
    "corr = df_data.select_dtypes(include = 'int64').corr()\n",
    "sns_ = sns.heatmap(corr, annot = True, cmap = 'YlGnBu')\n",
    "plt.setp(sns_.get_xticklabels(), rotation = 45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder_feat = {}\n",
    "for feature in ['drugname', 'condition']:\n",
    "    label_encoder_feat[feature] = LabelEncoder()\n",
    "    df_data[feature] = label_encoder_feat[feature].fit_transform(df_data[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries for the Machine Learning Model\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Features and splitting the data as train and test set\n",
    "\n",
    "features = df_data[['condition', 'usefulcount', 'sentiment_clean_ss',\n",
    "                   'count_word', 'count_unique_word', 'count_letters',\n",
    "                   'count_punctuations', 'count_words_upper', 'count_words_title',\n",
    "                   'count_stopwords', 'mean_word_len']]\n",
    "\n",
    "target = df_data['sentiment_rate']\n",
    "\n",
    "X_train, X_test, y_train,y_test = train_test_split(features, target, test_size = 0.3,random_state = 42)\n",
    "print (\"The Train set size \", X_train.shape)\n",
    "print (\"The Test set size \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of infinite values.\n",
    "\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "y_train.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of null values and missing values\n",
    "\n",
    "X_train.fillna(X_train.mean(), inplace=True)\n",
    "y_train.fillna(y_train.mean(), inplace=True)\n",
    "X_test.fillna(X_test.mean(), inplace=True)\n",
    "y_test.fillna(y_test.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Nural Network model\n",
    "\n",
    "MLP = MLPClassifier()\n",
    "MLP = MLP.fit(X_train,  y_train)\n",
    "y_pred = MLP.predict(X_test)\n",
    "y_pred\n",
    "score = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"Accuracy using Neural Network: \", round(score, 1), \"%\" )\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print(cm)\n",
    "\n",
    "#plot confusion Matrix\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "\n",
    "#Accuracy\n",
    "Accuracy = (tn+tp)*100/(tp+tn+fp+fn) \n",
    "print(\"Accuracy using Neural Network Classifier: \", round(Accuracy, 1), \"%\" )\n",
    "\n",
    "#Precision \n",
    "Precision = tp/(tp+fp) \n",
    "print(\"Precision using Neural Network Classifier: \", round(Precision, 1), \"%\" )\n",
    "\n",
    "#Recall \n",
    "Recall = tp/(tp+fn) \n",
    "print(\"Recall using Neural Network Classifier: \", round(Recall, 1), \"%\" )\n",
    "\n",
    "#F1 Score\n",
    "f1 = (2*Precision*Recall)/(Precision + Recall)\n",
    "print(\"F1 score using Neural Network Classifier: \", round(f1, 1), \"%\" )\n",
    "\n",
    "#F-beta score calculation\n",
    "def fbeta(precision, recall, beta):\n",
    "    return ((1+pow(beta,2))*precision*recall)/(pow(beta,2)*precision + recall)\n",
    "            \n",
    "f2 = fbeta(Precision, Recall, 2)\n",
    "print(\"F2 score using Neural Network Classifier: \", round(f2, 1), \"%\" )\n",
    "\n",
    "f0_5 = fbeta(Precision, Recall, 0.5)\n",
    "print(\"F0.5 score using Neural Network Classifier: \", round(f0_5, 1), \"%\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Naive Bayes model\n",
    "\n",
    "Nb=GaussianNB()\n",
    "Nb= Nb.fit(X_train,  y_train)\n",
    "y_pred=Nb.predict(X_test)\n",
    "y_pred\n",
    "score = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"Accuracy using Naive Bayes: \", round(score, 1), \"%\" )\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "#plot confusion Matrix\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "\n",
    "#Accuracy\n",
    "Accuracy = (tn+tp)*100/(tp+tn+fp+fn) \n",
    "print(\"Accuracy using Naive Bayes Classifier: \", round(Accuracy, 1), \"%\" )\n",
    "\n",
    "#Precision \n",
    "Precision = tp/(tp+fp) \n",
    "print(\"Precision using Naive Bayes Classifier: \", round(Precision, 1), \"%\" )\n",
    "\n",
    "#Recall \n",
    "Recall = tp/(tp+fn) \n",
    "print(\"Recall using Naive Bayes Classifier: \", round(Recall, 1), \"%\" )\n",
    "\n",
    "#F1 Score\n",
    "f1 = (2*Precision*Recall)/(Precision + Recall)\n",
    "print(\"F1 score using Naive Bayes Classifier: \", round(f1, 1), \"%\" )\n",
    "\n",
    "#F-beta score calculation\n",
    "def fbeta(precision, recall, beta):\n",
    "    return ((1+pow(beta,2))*precision*recall)/(pow(beta,2)*precision + recall)\n",
    "            \n",
    "f2 = fbeta(Precision, Recall, 2)\n",
    "print(\"F2 score using Naive Bayes Classifier: \", round(f2, 1), \"%\" )\n",
    "\n",
    "f0_5 = fbeta(Precision, Recall, 0.5)\n",
    "print(\"F0.5 score using Naive Bayes Classifier: \", round(f0_5, 1), \"%\" )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Decession tree Model\n",
    "\n",
    "dt= DecisionTreeClassifier(min_samples_split = 100, criterion='entropy')\n",
    "dt = dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "y_pred\n",
    "score = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"Accuracy using Desicion Tree: \", round(score, 1), \"%\" )\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print(cm)\n",
    "\n",
    "\n",
    "#plot confusion Matrix\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "\n",
    "#Accuracy\n",
    "Accuracy = (tn+tp)*100/(tp+tn+fp+fn) \n",
    "print(\"Accuracy using Desicion Tree Classifier: \", round(Accuracy, 1), \"%\" )\n",
    "\n",
    "#Precision \n",
    "Precision = tp/(tp+fp) \n",
    "print(\"Precision using Desicion Tree Classifier: \", round(Precision, 1), \"%\" )\n",
    "\n",
    "#Recall \n",
    "Recall = tp/(tp+fn) \n",
    "print(\"Recall using Desicion Tree Classifier: \", round(Recall, 1), \"%\" )\n",
    "\n",
    "#F1 Score\n",
    "f1 = (2*Precision*Recall)/(Precision + Recall)\n",
    "print(\"F1 score using Desicion Tree Classifier: \", round(f1, 1), \"%\" )\n",
    "\n",
    "#F-beta score calculation\n",
    "def fbeta(precision, recall, beta):\n",
    "    return ((1+pow(beta,2))*precision*recall)/(pow(beta,2)*precision + recall)\n",
    "            \n",
    "f2 = fbeta(Precision, Recall, 2)\n",
    "print(\"F2 score using Desicion Tree Classifier: \", round(f2, 1), \"%\" )\n",
    "\n",
    "f0_5 = fbeta(Precision, Recall, 0.5)\n",
    "print(\"F0.5 score using Desicion Tree Classifier: \", round(f0_5, 1), \"%\" )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Random Forest model\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "RF = RF.fit(X_train,  y_train)\n",
    "y_pred = RF.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "score = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"Accuracy using Random Forest Classifier: \", round(score, 1), \"%\" )\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print(cm)\n",
    "\n",
    "#plot confusion Matrix\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "\n",
    "#Accuracy\n",
    "Accuracy = (tn+tp)*100/(tp+tn+fp+fn) \n",
    "print(\"Accuracy using Random Forest Classifier: \", round(Accuracy, 1), \"%\" )\n",
    "\n",
    "#Precision \n",
    "Precision = tp/(tp+fp) \n",
    "print(\"Precision using Random Forest Classifier: \", round(Precision, 1), \"%\" )\n",
    "\n",
    "#Recall \n",
    "Recall = tp/(tp+fn) \n",
    "print(\"Recall using Random Forest Classifier: \", round(Recall, 1), \"%\" )\n",
    "\n",
    "#F1 Score\n",
    "f1 = (2*Precision*Recall)/(Precision + Recall)\n",
    "print(\"F1 score using Random Forest Classifier: \", round(f1, 1), \"%\" )\n",
    "\n",
    "#F-beta score calculation\n",
    "def fbeta(precision, recall, beta):\n",
    "    return ((1+pow(beta,2))*precision*recall)/(pow(beta,2)*precision + recall)\n",
    "            \n",
    "f2 = fbeta(Precision, Recall, 2)\n",
    "print(\"F2 score using Random Forest Classifier: \", round(f2, 1), \"%\" )\n",
    "\n",
    "f0_5 = fbeta(Precision, Recall, 0.5)\n",
    "print(\"F0.5 score using Random Forest Classifier: \", round(f0_5, 1), \"%\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Logistic Regression Model\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"Accuracy using Logistic Regression Classifier: \", round(score, 1), \"%\" )\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print(cm)\n",
    "\n",
    "#plot confusion Matrix\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "\n",
    "#Accuracy\n",
    "Accuracy = (tn+tp)*100/(tp+tn+fp+fn) \n",
    "print(\"Accuracy using Logistic Regression Classifier: \", round(Accuracy, 1), \"%\" )\n",
    "\n",
    "#Precision \n",
    "Precision = tp/(tp+fp) \n",
    "print(\"Precision using Logistic Regression Classifier: \", round(Precision, 1), \"%\" )\n",
    "\n",
    "#Recall \n",
    "Recall = tp/(tp+fn) \n",
    "print(\"Recall using Logistic Regression Classifier: \", round(Recall, 1), \"%\" )\n",
    "\n",
    "#F1 Score\n",
    "f1 = (2*Precision*Recall)/(Precision + Recall)\n",
    "print(\"F1 score using Logistic Regression Classifier: \", round(f1, 1), \"%\" )\n",
    "\n",
    "#F-beta score calculation\n",
    "def fbeta(precision, recall, beta):\n",
    "    return ((1+pow(beta,2))*precision*recall)/(pow(beta,2)*precision + recall)\n",
    "            \n",
    "f2 = fbeta(Precision, Recall, 2)\n",
    "print(\"F2 score using Logistic Regression Classifier: \", round(f2, 1), \"%\" )\n",
    "\n",
    "f0_5 = fbeta(Precision, Recall, 0.5)\n",
    "print(\"F0.5 score using Logistic Regression Classifier: \", round(f0_5, 1), \"%\" )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Support vector Machine(SVC) Model\n",
    "\n",
    "#preprocessing the Train And Test Data\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_train)\n",
    "X_train = scaling.transform(X_train)\n",
    "X_test = scaling.transform(X_test)\n",
    "\n",
    "# SVM Model\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train,y_train)\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred) * 100\n",
    "print(\"Accuracy using SVM Classifier: \", round(score, 1), \"%\" )\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print(cm)\n",
    "\n",
    "#plot confusion Matrix\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Class')\n",
    "plt.xlabel('Predicted Class')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "print(\"True Negatives: \",tn)\n",
    "print(\"False Positives: \",fp)\n",
    "print(\"False Negatives: \",fn)\n",
    "print(\"True Positives: \",tp)\n",
    "\n",
    "#Accuracy\n",
    "Accuracy = (tn+tp)*100/(tp+tn+fp+fn) \n",
    "print(\"Accuracy using  SVM Classifier: \", round(Accuracy, 1), \"%\" )\n",
    "\n",
    "#Precision \n",
    "Precision = tp/(tp+fp) \n",
    "print(\"Precision using  SVM Classifier: \", round(Precision, 1), \"%\" )\n",
    "\n",
    "#Recall \n",
    "Recall = tp/(tp+fn) \n",
    "print(\"Recall using  SVM Classifier: \", round(Recall, 1), \"%\" )\n",
    "\n",
    "#F1 Score\n",
    "f1 = (2*Precision*Recall)/(Precision + Recall)\n",
    "print(\"F1 score using  SVM Classifier: \", round(f1, 1), \"%\" )\n",
    "\n",
    "#F-beta score calculation\n",
    "def fbeta(precision, recall, beta):\n",
    "    return ((1+pow(beta,2))*precision*recall)/(pow(beta,2)*precision + recall)\n",
    "            \n",
    "f2 = fbeta(Precision, Recall, 2)\n",
    "print(\"F2 score using  SVM Classifier: \", round(f2, 1), \"%\" )\n",
    "\n",
    "f0_5 = fbeta(Precision, Recall, 0.5)\n",
    "print(\"F0.5 score using  SVM Classifier: \", round(f0_5, 1), \"%\" )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
